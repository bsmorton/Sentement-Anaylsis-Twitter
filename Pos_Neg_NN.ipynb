{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, SimpleRNN, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "###########################################################################\n",
    "#generate training/test data\n",
    "########################################################################### \n",
    "br_file = io.open(\"positive.txt\",'r',encoding=\"latin-1\")\n",
    "bi_file = io.open(\"negative.txt\",'r',encoding=\"latin-1\")\n",
    "\n",
    "\n",
    "\n",
    "br_tweets = []\n",
    "bi_tweets = []\n",
    "\n",
    "for _ in range(250000):\n",
    "    br_tweets.append([br_file.readline().strip(\"\\n\"),0])\n",
    "    bi_tweets.append([bi_file.readline().strip(\"\\n\"),1])\n",
    "    \n",
    "tweets = br_tweets+bi_tweets\n",
    "np.random.shuffle(tweets)\n",
    "\n",
    "#1350000\n",
    "#945000\n",
    "tweets_train = np.array(tweets[:300000])\n",
    "tweets_test = np.array(tweets[300000:])\n",
    "\n",
    "X_train = tweets_train[:,0]\n",
    "Y_train = to_categorical(np.array([int(i) for i in tweets_train[:,1]],dtype=np.int))\n",
    "X_test = tweets_test[:,0]\n",
    "Y_test = to_categorical(np.array([int(i) for i in tweets_test[:,1]],dtype=np.int))\n",
    "\n",
    "test_tweets = [\"Fuck you mom\",\"I love you mom\",\"I hate homework\",\"I enjoy homework\", \"Life sucks\",\"Life is good\",\n",
    "               \"Jesus loves me\",\"Put a ring on it\",\"Mercedes Benz: $20,000\",\n",
    "                \"The decision on Sergeant Bergdahl is a complete and total disgrace to our Country and to our Military.\",\n",
    "                \"Does anybody really believe that a reporter, who nobody ever heard of, went to his mailbox and found my tax returns? @NBCNews FAKE NEWS!\",\n",
    "                \"After Turkey call I will be heading over to Trump National Golf Club, Jupiter, to play golf (quickly) with Tiger Woods and Dustin Johnson. Then back to Mar-a-Lago for talks on bringing even more jobs and companies back to the USA!\",\n",
    "                \"“The President has accomplished some absolutely historic things during this past year.” Thank you Charlie Kirk of Turning Points USA. Sadly, the Fake Mainstream Media will NEVER talk about our accomplishments in their end of year reviews. We are compiling a long & beautiful list.\",\n",
    "                \"Crazy Joe Scarborough and dumb as a rock Mika are not bad people, but their low rated show is dominated by their NBC bosses. Too bad!\",\n",
    "                \"Robert Pattinson should not take back Kristen Stewart. She cheated on him like a dog & will do it again--just watch. He can do much better!\"]\n",
    "\n",
    "#########################################################################\n",
    "#bag of words\n",
    "#########################################################################\n",
    "\n",
    "vectorizer = CountVectorizer( )\n",
    "X_train_counts = vectorizer.fit_transform(X_train)\n",
    "X_test_counts = vectorizer.transform(X_test)\n",
    "test_tweets_counts = vectorizer.transform(test_tweets)\n",
    "\n",
    "\n",
    "print(\"SUCCESS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntop_words = 10000\\nmax_tweet_length = 50\\n\\ntokenizer = Tokenizer(num_words = top_words)\\ntokenizer.fit_on_texts(X_train)\\ntokenizer.fit_on_texts(X_test)\\ntokenizer.fit_on_texts(test_tweets)\\nsequences = tokenizer.texts_to_sequences(X_train)\\nword_index = tokenizer.word_index\\nprint ('Found %s unique tokens.' % len(word_index))\\n\\nX_train = sequence.pad_sequences(sequences, maxlen=max_tweet_length)\\nY_train = np.asarray(Y_train)\\n\\n\\n\\n\\n\\n#X_test = sequence.pad_sequences(X_test_counts, maxlen=max_tweet_length)\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 250)               52256750  \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 502       \n",
      "=================================================================\n",
      "Total params: 52,257,252\n",
      "Trainable params: 52,257,252\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 240000 samples, validate on 60000 samples\n",
      "Epoch 1/9\n",
      " 29120/240000 [==>...........................] - ETA: 1:03:48 - loss: 0.5790 - acc: 0.7077"
     ]
    }
   ],
   "source": [
    "import keras.optimizers\n",
    "\n",
    "\n",
    "model = Sequential() \n",
    "model.add( Dense(250, activation=\"selu\", input_dim= X_train_counts.shape[1]) )\n",
    "model.add( Dense(units=2, activation=\"softmax\"))\n",
    "\n",
    "sgd = keras.optimizers.SGD(lr=0.0001*400)\n",
    "model.compile(optimizer=sgd,loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "history = model.fit(X_train_counts, Y_train, epochs=9, batch_size=32, validation_split = 0.2)\n",
    "model.save_weights('pos_neg_NN_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) +1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label = 'Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label = 'Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label = 'Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label = 'Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('pos_neg_NN_model.h5')\n",
    "print(\"Classification Error on Test Set: \",model.evaluate(X_test_counts,Y_test,verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "predictions = model.predict(test_tweets_counts)\n",
    "for i in range(len(predictions)):\n",
    "    print(test_tweets[i],\"\\n\",predictions[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
